{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ShenChenchen/.conda/envs/UnbiasedWatermark/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "from random import shuffle\n",
    "from pprint import pprint\n",
    "sys.path.insert(0, '../../KGWatermark') \n",
    "sys.path.insert(0, '../../KTHwatermark/demo')\n",
    "sys.path.insert(0, '../../PRWWatermark')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing specifically from transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    HfArgumentParser,\n",
    "    LogitsProcessorList,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# Importing specifically from your extended watermark processor module\n",
    "from extended_watermark_processor import WatermarkLogitsProcessor as KGW, WatermarkDetector as KGWDetector\n",
    "from generate import *\n",
    "from gptwm import GPTWatermarkLogitsWarper\n",
    "\n",
    "\n",
    "# Importing from langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Importing exception handling for requests\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "# Setting device for PyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Load json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        file = json.load(f)\n",
    "        f.close()\n",
    "    return file\n",
    "\n",
    "def save_json_file(file, file_path):\n",
    "    \"\"\"\n",
    "    Save json file\n",
    "    \"\"\"\n",
    "    with open(file_path,'w',encoding='utf-8') as f:\n",
    "        json.dump(file, f, indent=4, ensure_ascii=False)\n",
    "        f.close()\n",
    "\n",
    "def gen(model, tokenizer, prompt, max_length, temperature = 0.35, watermark = None):\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens = max_length,\n",
    "        repetition_penalty = 1.0,\n",
    "        do_sample = True,\n",
    "        #  num_beams = 5,\n",
    "        temperature = temperature,\n",
    "        # num_return_sequences=5,\n",
    "        early_stopping=True, \n",
    "        no_repeat_ngram_size=4, \n",
    "        pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    model_inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors=\"pt\",max_length=1024)\n",
    "    # print(model_inputs['attention_mask'])\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_inputs.to(device)\n",
    "    if watermark is not None:\n",
    "        outputs = model.generate(**model_inputs,\n",
    "                                 logits_processor = LogitsProcessorList([watermark]),\n",
    "                                generation_config = generation_config)\n",
    "    else:\n",
    "        outputs = model.generate(**model_inputs,\n",
    "                                generation_config = generation_config)\n",
    "    result = []\n",
    "    for item in outputs:\n",
    "        # print(item)\n",
    "        result.append(tokenizer.decode(item, skip_special_tokens=True))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since allenai/c4 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'realnewslike' at /home/ShenChenchen/.cache/huggingface/datasets/allenai___c4/realnewslike/0.0.0/1588ec454efa1a09f29cd18ddd04fe05fc8653a2 (last modified on Tue Mar 26 12:00:09 2024).\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"allenai/c4\", \"realnewslike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", device_map = 'auto', padding_side = 'left', add_eos_token=True, add_bos_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\",quantization_config = bnb_config, device_map = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "KGW_1 = KGW(vocab=list(tokenizer.get_vocab().values()),\n",
    "            gamma=0.25,\n",
    "            delta=2.0,\n",
    "            seeding_scheme=\"selfhash\") #equivalent to `ff-anchored_minhash_prf-4-True-15485863`\n",
    "KGW_detector = KGWDetector(vocab=list(tokenizer.get_vocab().values()),\n",
    "                            gamma=0.25, # should match original setting\n",
    "                            seeding_scheme=\"selfhash\", # should match original setting\n",
    "                            device = device, # must match the original rng device type\n",
    "                            tokenizer=tokenizer,\n",
    "                            z_threshold=4.0,\n",
    "                            normalizers=[],\n",
    "                            ignore_repeated_ngrams=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create truncated dataset (Token-wize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Tokenization...\n",
      "End Tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 13863/13863 [00:01<00:00, 10619.62it/s]\n",
      "100%|██████████| 13863/13863 [00:00<00:00, 25233.78it/s]\n"
     ]
    }
   ],
   "source": [
    "truncated_tokenized_tensors = []\n",
    "threshold = 70\n",
    "\n",
    "print(\"Start Tokenization...\")\n",
    "tokenized_tensors = list(map(lambda s: tokenizer(s, return_tensors='pt'), dataset['validation']['text']))\n",
    "print(\"End Tokenization...\")\n",
    "\n",
    "truncated_tokenized_tensors = []\n",
    "decoded_text_inputs = []\n",
    "for item in tqdm(tokenized_tensors, desc=\"Processing\"):\n",
    "    truncated_tokenized_tensors.append({'input_ids':item['input_ids'][:, :threshold].to(device), \n",
    "                                        'attention_mask': item['attention_mask'][:,:threshold].to(device)})\n",
    "\n",
    "for item in tqdm(truncated_tokenized_tensors):\n",
    "    # Assuming 'input_ids' are on the CPU for decoding\n",
    "    input_ids = item['input_ids'][0].cpu().numpy()\n",
    "    decoded_text_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    decoded_text_inputs.append(decoded_text_input)\n",
    "\n",
    "assert(len(decoded_text_inputs) == len(truncated_tokenized_tensors)), \"There's something wrong with decoding.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraphrase preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ShenChenchen/.conda/envs/UnbiasedWatermark/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ShenChenchen/.conda/envs/UnbiasedWatermark/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Introduce Berkeley to me in general...and here's the rest of the band:\\n\"\n",
      " \" nobody's fool\\n\"\n",
      " '\\n'\n",
      " 'The Berkeley band is a classic American rock band known for their energetic '\n",
      " \"live performances and catchy hooks. The band's music is a mix of rock, \"\n",
      " 'blues, and soul, and their songs often explore themes of love, '\n",
      " 'relationships, and the human condition.\\n'\n",
      " '\\n'\n",
      " \"The band's lead singer, John, is a charismatic frontman with a powerful \"\n",
      " 'voice and a']\n"
     ]
    }
   ],
   "source": [
    "paraphrase_prompt = '''\n",
    "«SYS»\n",
    "Assume you are a helpful assistant. Your job is to paraphrase the given text.\n",
    "«/SYS»\n",
    "[INST]\n",
    "{INPUT_TEXT}\n",
    "[/INST]\n",
    "Response template:\n",
    "\"You’re welcome! Here’s a paraphrased version of the original message: {PARAPHRASED_TEXT}\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRWWatermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing item 9: 100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "data = range(10)\n",
    "pbar = tqdm(total=len(data))\n",
    "\n",
    "for i in data:\n",
    "    pbar.set_description(f\"Processing item {i}\")\n",
    "    time.sleep(0.5)  # Simulate some processing time\n",
    "    pbar.update()  # Manually update the progress bar\n",
    "\n",
    "pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PRW_1 \u001b[38;5;241m=\u001b[39m LogitsProcessorList([GPTWatermarkLogitsWarper(fraction\u001b[38;5;241m=\u001b[39m\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mfraction,\n\u001b[1;32m      2\u001b[0m                                                                         strength\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mstrength,\n\u001b[1;32m      3\u001b[0m                                                                         vocab_size\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m      4\u001b[0m                                                                         watermark_key\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mwm_key)])\n\u001b[1;32m      5\u001b[0m generation_args \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "PRW_1 = LogitsProcessorList([GPTWatermarkLogitsWarper(fraction=0.5,\n",
    "                                                    strength=2.0,\n",
    "                                                    vocab_size=model.config.vocab_size,\n",
    "                                                    watermark_key=0)])\n",
    "\n",
    "\n",
    "generation_args = {\n",
    "    'logits_processor': PRW_1,\n",
    "    'output_scores': True,\n",
    "    'return_dict_in_generate': True,\n",
    "    'max_new_tokens': 300,\n",
    "    'num_beams': None,\n",
    "    'do_sample': True,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KGWatermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduce Berkeley to me*  I appreciate your response and I will make sure to clearly specify the use of the term \"Berkeley\" in the future.\n",
      "\n",
      "The term \"Berkeley\" can refer to several different things depending on the context:\n",
      "\n",
      "1.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "tokens = tokenizer.encode(\"Introduce Berkeley to me\", return_tensors='pt', truncation=True, max_length=2048)\n",
    "\n",
    "watermarked_tokens = generate_shift(model,tokens,len(tokenizer),40,50,42)[0]\n",
    "watermarked_text = tokenizer.decode(watermarked_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(watermarked_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UnbiasedWatermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
